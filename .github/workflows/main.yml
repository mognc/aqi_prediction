name: AQI Project Pipeline

on:
  push:
    branches:
      - main
  schedule:
    - cron: "0 * * * *" # Every hour
  workflow_dispatch: # For manual triggering

jobs:
  run-airflow-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v2

      - name: Set Up Python
        uses: actions/setup-python@v3
        with:
          python-version: 3.9
          

      - name: Cache Dependencies
        uses: actions/cache@v2
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Intsall grpcio and googleapis
        run: |
          pip install grpcio
          pip install googleapis-common-protos

      - name: Install Dependencies
        run: |
          pip install apache-airflow
          pip install feast==0.42.0
          pip install -r dags/scripts/requirements.txt

      - name: Initialize Airflow Database
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow
          airflow db migrate

      - name: List DAGs
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow
          export AIRFLOW__CORE__DAGS_FOLDER=$(pwd)/dags
          airflow dags list
        shell: /usr/bin/bash -e {0}
        env:
          AIRFLOW_HOME: ${{ github.workspace }}/airflow
          AIRFLOW__CORE__DAGS_FOLDER: ${{ github.workspace }}/dags
      
      - name: Run Airflow DAG
        run: |
          export AIRFLOW_HOME=$(pwd)/airflow
          export AIRFLOW__CORE__DAGS_FOLDER=$(pwd)/dags
          airflow dags trigger feature_and_training_pipeline
        shell: /usr/bin/bash -e {0}
        env:
          AIRFLOW_HOME: ${{ github.workspace }}/airflow
          AIRFLOW__CORE__DAGS_FOLDER: ${{ github.workspace }}/dags

